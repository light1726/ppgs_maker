# steps/nnet/train.sh --feature-transform exp/dnn4_pretrain-dbn/final.feature_transform --dbn exp/dnn4_pretrain-dbn/4.dbn --hid-layers 0 --learn-rate 0.008 data-fmllr-tri3/train_tr90 data-fmllr-tri3/train_cv10 data/lang exp/tri3_ali exp/tri3_ali exp/dnn4_pretrain-dbn_dnn 
# Started at Thu Jun 21 10:45:48 CST 2018
#
steps/nnet/train.sh --feature-transform exp/dnn4_pretrain-dbn/final.feature_transform --dbn exp/dnn4_pretrain-dbn/4.dbn --hid-layers 0 --learn-rate 0.008 data-fmllr-tri3/train_tr90 data-fmllr-tri3/train_cv10 data/lang exp/tri3_ali exp/tri3_ali exp/dnn4_pretrain-dbn_dnn

# INFO
steps/nnet/train.sh : Training Neural Network
	 dir       : exp/dnn4_pretrain-dbn_dnn 
	 Train-set : data-fmllr-tri3/train_tr90 3328, exp/tri3_ali 
	 CV-set    : data-fmllr-tri3/train_cv10 368 exp/tri3_ali 

LOG ([5.4.140~1-1ab3d]:main():cuda-gpu-available.cc:49) 

### IS CUDA GPU AVAILABLE? 'mjrc-server10' ###
WARNING ([5.4.140~1-1ab3d]:SelectGpuId():cu-device.cc:196) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:315) Selecting from 4 GPUs
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:330) cudaSetDevice(0): GeForce GTX 1080	free:7833M, used:286M, total:8119M, free/total:0.964776
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:330) cudaSetDevice(1): GeForce GTX 1080	free:7833M, used:286M, total:8119M, free/total:0.964776
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:330) cudaSetDevice(2): GeForce GTX 1080	free:86M, used:8033M, total:8119M, free/total:0.010661
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:330) cudaSetDevice(3): GeForce GTX 1080	free:242M, used:7877M, total:8119M, free/total:0.0298738
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:379) Trying to select device: 0 (automatically), mem_ratio: 0.964776
LOG ([5.4.140~1-1ab3d]:SelectGpuIdAuto():cu-device.cc:398) Success selecting device 0 free mem ratio: 0.964776
LOG ([5.4.140~1-1ab3d]:FinalizeActiveGpu():cu-device.cc:247) The active GPU is [0]: GeForce GTX 1080	free:7769M, used:350M, total:8119M, free/total:0.956894 version 6.1
### HURRAY, WE GOT A CUDA GPU FOR COMPUTATION!!! ##

### Testing CUDA setup with a small computation (setup = cuda-toolkit + gpu-driver + kaldi):
### Test OK!

# PREPARING ALIGNMENTS
Using PDF targets from dirs 'exp/tri3_ali' 'exp/tri3_ali'
hmm-info exp/tri3_ali/final.mdl 
copy-transition-model --binary=false exp/tri3_ali/final.mdl exp/dnn4_pretrain-dbn_dnn/final.mdl 
LOG (copy-transition-model[5.4.140~1-1ab3d]:main():copy-transition-model.cc:62) Copied transition model.

# PREPARING FEATURES
# re-saving features to local disk,
copy-feats --compress=true scp:data-fmllr-tri3/train_tr90/feats.scp ark,scp:/tmp/kaldi.Plat/train.ark,exp/dnn4_pretrain-dbn_dnn/train_sorted.scp 
LOG (copy-feats[5.4.140~1-1ab3d]:main():copy-feats.cc:143) Copied 3328 feature matrices.
copy-feats --compress=true scp:data-fmllr-tri3/train_cv10/feats.scp ark,scp:/tmp/kaldi.Plat/cv.ark,exp/dnn4_pretrain-dbn_dnn/cv.scp 
LOG (copy-feats[5.4.140~1-1ab3d]:main():copy-feats.cc:143) Copied 368 feature matrices.
# importing feature settings from dir 'exp/dnn4_pretrain-dbn'
# cmvn_opts='' delta_opts='' ivector_dim=''
# 'apply-cmvn' is not used,
feat-to-dim 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- |' - 
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- 
WARNING (feat-to-dim[5.4.140~1-1ab3d]:Close():kaldi-io.cc:515) Pipe copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- | had nonzero return status 13
# feature dim : 40 (input of 'feature_transform')
# importing 'feature_transform' from 'exp/dnn4_pretrain-dbn/final.feature_transform'

### Showing the final 'feature_transform':
nnet-info exp/dnn4_pretrain-dbn_dnn/imported_final.feature_transform 
LOG (nnet-info[5.4.140~1-1ab3d]:main():nnet-info.cc:57) Printed info about exp/dnn4_pretrain-dbn_dnn/imported_final.feature_transform
num-components 3
input-dim 40
output-dim 440
number-of-parameters 0.00088 millions
component 1 : <Splice>, input-dim 40, output-dim 440, 
  frame_offsets [ -5 -4 -3 -2 -1 0 1 2 3 4 5 ]
component 2 : <AddShift>, input-dim 440, output-dim 440, 
  shift_data ( min -0.0793826, max 0.111283, mean -0.00415868, stddev 0.0315898, skewness 1.01899, kurtosis 3.9731 ) , lr-coef 0
component 3 : <Rescale>, input-dim 440, output-dim 440, 
  scale_data ( min 0.363491, max 1.0482, mean 0.823978, stddev 0.160735, skewness -0.795709, kurtosis -0.00400281 ) , lr-coef 0
###

# NN-INITIALIZATION
# getting input/output dims :
feat-to-dim 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- | nnet-forward "nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform '\''exp/dnn4_pretrain-dbn/4.dbn'\'' -|" ark:- ark:- |' - 
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- 
nnet-forward "nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'exp/dnn4_pretrain-dbn/4.dbn' -|" ark:- ark:- 
LOG (nnet-forward[5.4.140~1-1ab3d]:SelectGpuId():cu-device.cc:123) Manually selected to compute on CPU.
nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform exp/dnn4_pretrain-dbn/4.dbn - 
LOG (nnet-concat[5.4.140~1-1ab3d]:main():nnet-concat.cc:53) Reading exp/dnn4_pretrain-dbn_dnn/final.feature_transform
LOG (nnet-concat[5.4.140~1-1ab3d]:main():nnet-concat.cc:65) Concatenating exp/dnn4_pretrain-dbn/4.dbn
LOG (nnet-concat[5.4.140~1-1ab3d]:main():nnet-concat.cc:82) Written model to -
WARNING (feat-to-dim[5.4.140~1-1ab3d]:Close():kaldi-io.cc:515) Pipe copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- | nnet-forward "nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'exp/dnn4_pretrain-dbn/4.dbn' -|" ark:- ark:- | had nonzero return status 36096
# genrating network prototype exp/dnn4_pretrain-dbn_dnn/nnet.proto
['utils/nnet/make_nnet_proto.py', '1024', '128', '0', '1024']
# initializing the NN 'exp/dnn4_pretrain-dbn_dnn/nnet.proto' -> 'exp/dnn4_pretrain-dbn_dnn/nnet.init'
nnet-initialize --seed=777 exp/dnn4_pretrain-dbn_dnn/nnet.proto exp/dnn4_pretrain-dbn_dnn/nnet.init 
VLOG[1] (nnet-initialize[5.4.140~1-1ab3d]:Init():nnet-nnet.cc:314) <AffineTransform> <InputDim> 1024 <OutputDim> 128 <BiasMean> 0.000000 <BiasRange> 0.000000 <ParamStddev> 0.145833
VLOG[1] (nnet-initialize[5.4.140~1-1ab3d]:Init():nnet-nnet.cc:314) <Softmax> <InputDim> 128 <OutputDim> 128
VLOG[1] (nnet-initialize[5.4.140~1-1ab3d]:Init():nnet-nnet.cc:314) </NnetProto>
LOG (nnet-initialize[5.4.140~1-1ab3d]:main():nnet-initialize.cc:63) Written initialized model to exp/dnn4_pretrain-dbn_dnn/nnet.init
nnet-concat exp/dnn4_pretrain-dbn/4.dbn exp/dnn4_pretrain-dbn_dnn/nnet.init exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init 
LOG (nnet-concat[5.4.140~1-1ab3d]:main():nnet-concat.cc:53) Reading exp/dnn4_pretrain-dbn/4.dbn
LOG (nnet-concat[5.4.140~1-1ab3d]:main():nnet-concat.cc:65) Concatenating exp/dnn4_pretrain-dbn_dnn/nnet.init
LOG (nnet-concat[5.4.140~1-1ab3d]:main():nnet-concat.cc:82) Written model to exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init

# RUNNING THE NN-TRAINING SCHEDULER
steps/nnet/train_scheduler.sh --feature-transform exp/dnn4_pretrain-dbn_dnn/final.feature_transform --learn-rate 0.008 exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- | ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- | ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | exp/dnn4_pretrain-dbn_dnn
CROSSVAL PRERUN AVG.LOSS 5.4845 (Xent),
ITERATION 01: TRAIN AVG.LOSS 0.9961, (lrate0.008), CROSSVAL AVG.LOSS 1.1858, nnet accepted (nnet_dbn_dnn_iter01_learnrate0.008_tr0.9961_cv1.1858)
ITERATION 02: TRAIN AVG.LOSS 0.7891, (lrate0.008), CROSSVAL AVG.LOSS 1.1557, nnet accepted (nnet_dbn_dnn_iter02_learnrate0.008_tr0.7891_cv1.1557)
ITERATION 03: TRAIN AVG.LOSS 0.6924, (lrate0.008), CROSSVAL AVG.LOSS 1.1737, nnet rejected (nnet_dbn_dnn_iter03_learnrate0.008_tr0.6924_cv1.1737_rejected)
ITERATION 04: TRAIN AVG.LOSS 0.6994, (lrate0.004), CROSSVAL AVG.LOSS 1.0899, nnet accepted (nnet_dbn_dnn_iter04_learnrate0.004_tr0.6994_cv1.0899)
ITERATION 05: TRAIN AVG.LOSS 0.6565, (lrate0.002), CROSSVAL AVG.LOSS 1.0421, nnet accepted (nnet_dbn_dnn_iter05_learnrate0.002_tr0.6565_cv1.0421)
ITERATION 06: TRAIN AVG.LOSS 0.6387, (lrate0.001), CROSSVAL AVG.LOSS 1.0040, nnet accepted (nnet_dbn_dnn_iter06_learnrate0.001_tr0.6387_cv1.0040)
ITERATION 07: TRAIN AVG.LOSS 0.6316, (lrate0.0005), CROSSVAL AVG.LOSS 0.9747, nnet accepted (nnet_dbn_dnn_iter07_learnrate0.0005_tr0.6316_cv0.9747)
ITERATION 08: TRAIN AVG.LOSS 0.6289, (lrate0.00025), CROSSVAL AVG.LOSS 0.9549, nnet accepted (nnet_dbn_dnn_iter08_learnrate0.00025_tr0.6289_cv0.9549)
ITERATION 09: TRAIN AVG.LOSS 0.6278, (lrate0.000125), CROSSVAL AVG.LOSS 0.9444, nnet accepted (nnet_dbn_dnn_iter09_learnrate0.000125_tr0.6278_cv0.9444)
ITERATION 10: TRAIN AVG.LOSS 0.6272, (lrate6.25e-05), CROSSVAL AVG.LOSS 0.9398, nnet accepted (nnet_dbn_dnn_iter10_learnrate6.25e-05_tr0.6272_cv0.9398)
ITERATION 11: TRAIN AVG.LOSS 0.6267, (lrate3.125e-05), CROSSVAL AVG.LOSS 0.9377, nnet accepted (nnet_dbn_dnn_iter11_learnrate3.125e-05_tr0.6267_cv0.9377)
ITERATION 12: TRAIN AVG.LOSS 0.6263, (lrate1.5625e-05), CROSSVAL AVG.LOSS 0.9370, nnet accepted (nnet_dbn_dnn_iter12_learnrate1.5625e-05_tr0.6263_cv0.9370)
finished, too small rel. improvement 0.000756096
steps/nnet/train_scheduler.sh: Succeeded training the Neural Network : 'exp/dnn4_pretrain-dbn_dnn/final.nnet'
steps/nnet/train.sh: Successfuly finished. 'exp/dnn4_pretrain-dbn_dnn'
# Removing features tmpdir /tmp/kaldi.Plat @ mjrc-server10
cv.ark
train.ark
# Accounting: time=396 threads=1
# Ended (code 0) at Thu Jun 21 10:52:24 CST 2018, elapsed time 396 seconds
